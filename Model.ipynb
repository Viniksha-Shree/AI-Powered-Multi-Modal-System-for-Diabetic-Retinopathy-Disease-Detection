{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c9b24-cbb2-4c02-9003-261d12bd893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label from text: Moderate DR\n",
      "Predicted label from image: Moderate DR\n",
      "Predicted label from both modalities: Moderate DR\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BartModel, BartTokenizer\n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "\n",
    "# Define the MultiModalClassifier class\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, text_model, image_model, text_feat_dim, image_feat_dim, hidden_dim, num_classes):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "        # Linear layers to project modality-specific features to a common hidden space\n",
    "        self.text_fc = nn.Linear(text_feat_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_feat_dim, hidden_dim)\n",
    "\n",
    "        # Final classifier head that outputs logits for each class\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, text_input=None, image_input=None):\n",
    "        features = None\n",
    "\n",
    "        if text_input is not None:\n",
    "            # Remove 'labels' if present so that BART doesn't receive an unexpected key\n",
    "            text_input_filtered = {k: v for k, v in text_input.items() if k != \"labels\"}\n",
    "            text_outputs = self.text_model(**text_input_filtered)\n",
    "            # Mean pooling over the sequence dimension (dim=1)\n",
    "            pooled_text = text_outputs.last_hidden_state.mean(dim=1)\n",
    "            text_features = self.text_fc(pooled_text)\n",
    "            features = text_features if features is None else features + text_features\n",
    "\n",
    "        if image_input is not None:\n",
    "            # The image model should output a feature vector (with final fc replaced by Identity)\n",
    "            image_features = self.image_model(image_input)\n",
    "            image_features = self.image_fc(image_features)\n",
    "            features = image_features if features is None else features + image_features\n",
    "\n",
    "        # If both modalities are provided, average their features\n",
    "        if (text_input is not None) and (image_input is not None):\n",
    "            features = features / 2\n",
    "\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "# Load the tokenizer\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define image transformations for inference\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the CSV file to get the label list\n",
    "text_df = pd.read_csv(\"dataset.csv\")\n",
    "label_list = text_df['label'].unique().tolist()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"multimodal_model.pth\"\n",
    "\n",
    "# Load the pre-trained ResNet18 model and replace its fc layer with Identity\n",
    "image_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "image_model.fc = nn.Identity()\n",
    "\n",
    "# Create the MultiModalClassifier using the modified image model\n",
    "model = MultiModalClassifier(\n",
    "    text_model=BartModel.from_pretrained(model_name),\n",
    "    image_model=image_model,\n",
    "    text_feat_dim=768,   # typically 768 for bart-base\n",
    "    image_feat_dim=512,  # typically 512 for resnet18 after replacing fc\n",
    "    hidden_dim=512,\n",
    "    num_classes=len(label_list)\n",
    ")\n",
    "\n",
    "# Load the saved state dict\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Mapping from label IDs to label names\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "def inference_text(model, tokenizer, text, device, max_length=128):\n",
    "    \"\"\"Perform inference on text input only.\"\"\"\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Move encoding to device\n",
    "    for key in encoding:\n",
    "        encoding[key] = encoding[key].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=encoding, image_input=None)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "def inference_image(model, image_path, transform, device):\n",
    "    \"\"\"Perform inference on an image input only.\"\"\"\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # add batch dimension\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=None, image_input=image)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "def inference_both(model, tokenizer, text, image_path, transform, device, max_length=128):\n",
    "    \"\"\"Perform inference using both text and image inputs.\"\"\"\n",
    "    model.eval()\n",
    "    # Process text input\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    for key in encoding:\n",
    "        encoding[key] = encoding[key].to(device)\n",
    "    # Process image input\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=encoding, image_input=image)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "# Example inference usage\n",
    "sample_text = \"Increased microaneurysms, Cotton wool spots, Mild vision loss, Venous beading, Intraretinal microvascular abnormalities, Hard exudates\"\n",
    "sample_image_path = r\"C:\\Users\\home\\OneDrive\\Desktop\\Final Year Project\\MULITEMODEL_AI_EYE_DIS\\TRAIN_CODE\\dataset\\train\\Moderate DR\\Moderate_DR_2.png\"\n",
    "\n",
    "# Text inference\n",
    "predicted_label_text = inference_text(model, tokenizer, sample_text, device)\n",
    "print(f\"Predicted label from text: {predicted_label_text}\")\n",
    "\n",
    "# Image inference\n",
    "predicted_label_image = inference_image(model, sample_image_path, image_transforms, device)\n",
    "print(f\"Predicted label from image: {predicted_label_image}\")\n",
    "\n",
    "# Combined inference\n",
    "predicted_label_both = inference_both(model, tokenizer, sample_text, sample_image_path, image_transforms, device)\n",
    "print(f\"Predicted label from both modalities: {predicted_label_both}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea5bd4-ddd6-4d55-a6ea-6ad362e075a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
